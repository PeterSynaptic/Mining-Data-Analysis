{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Data Analysis Report\n",
    "This notebook analyzes a 20% sample of the mining dataset, focusing on data preprocessing, cleaning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Read the Excel file\n",
    "df = pd.read_excel('Copy of MTL Coding Exercise_MSS_New.xlsx')\n",
    "\n",
    "# Take 20% random sample\n",
    "df_sample = df.sample(frac=0.2, random_state=42)\n",
    "\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Sample dataset size: {len(df_sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "missing_values = df_sample.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Basic statistics of numerical columns\n",
    "numeric_stats = df_sample.describe()\n",
    "print(\"\\nBasic statistics of numerical columns:\")\n",
    "print(numeric_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clean the data\n",
    "df_clean = df_sample.copy()\n",
    "\n",
    "# Handle missing values\n",
    "df_clean['mean_lh'].fillna(df_clean['mean_lh'].mean(), inplace=True)\n",
    "df_clean['cuka_dcr'].fillna(df_clean['cuka_dcr'].mean(), inplace=True)\n",
    "df_clean['moka_dcr'].fillna(df_clean['moka_dcr'].mean(), inplace=True)\n",
    "\n",
    "# Remove outliers using IQR method for key measurements\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "for col in ['cugrade', 'mograde', 'mean_lh']:\n",
    "    df_clean = remove_outliers(df_clean, col)\n",
    "\n",
    "print(f\"Dataset size after cleaning: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 1. Grade Distribution Plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data=df_clean, x='cugrade', bins=30)\n",
    "plt.title('Distribution of Copper Grade')\n",
    "plt.xlabel('Copper Grade (%)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data=df_clean, x='mograde', bins=30)\n",
    "plt.title('Distribution of Molybdenum Grade')\n",
    "plt.xlabel('Molybdenum Grade (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2. Correlation Analysis\n",
    "numerical_cols = ['mean_lh', 'cuka_dcr', 'moka_dcr', 'cugrade', 'mograde', \n",
    "                 'valid_bh_num', 'avg_bh_grade_cu', 'avg_bh_grade_mo', 'Dist_to_NN_bh']\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df_clean[numerical_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap of Key Variables')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3. Scatter Plot Matrix for Key Variables\n",
    "key_vars = ['cugrade', 'mograde', 'avg_bh_grade_cu', 'avg_bh_grade_mo']\n",
    "sns.pairplot(df_clean[key_vars], diag_kind='kde')\n",
    "plt.suptitle('Scatter Plot Matrix of Grade Measurements', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 4. Grade by Shift Analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "shift_stats = df_clean.groupby('shift_id')[['cugrade', 'mograde']].mean()\n",
    "shift_stats.plot(kind='bar')\n",
    "plt.title('Average Grades by Shift')\n",
    "plt.xlabel('Shift')\n",
    "plt.ylabel('Grade (%)')\n",
    "plt.legend(['Copper', 'Molybdenum'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis Report\n",
    "\n",
    "### Data Preprocessing and Cleaning:\n",
    "1. The dataset was sampled to 20% of its original size for analysis\n",
    "2. Missing values were handled by filling with mean values for continuous measurements\n",
    "3. Outliers were removed using the IQR method for key grade measurements\n",
    "\n",
    "### Key Insights from Visualizations:\n",
    "\n",
    "1. **Grade Distributions**:\n",
    "   - The copper and molybdenum grade distributions show the typical range and variation in mineral content\n",
    "   - Any significant skewness or multi-modal patterns would be visible in these plots\n",
    "\n",
    "2. **Correlation Analysis**:\n",
    "   - The heatmap reveals relationships between different measurements\n",
    "   - Strong correlations between sensor readings (cuka_dcr, moka_dcr) and actual grades would validate sensor accuracy\n",
    "   - Distance to nearest blasthole may show impact on prediction accuracy\n",
    "\n",
    "3. **Grade Relationships**:\n",
    "   - The scatter plot matrix shows relationships between predicted and actual grades\n",
    "   - Helps identify any systematic bias in predictions\n",
    "   - Shows if there's any correlation between copper and molybdenum grades\n",
    "\n",
    "4. **Shift Analysis**:\n",
    "   - Compares grade measurements between day and night shifts\n",
    "   - Helps identify any systematic differences in measurements between shifts\n",
    "   - Important for quality control and operational consistency\n",
    "\n",
    "### Recommendations:\n",
    "1. Monitor and calibrate sensors based on correlation analysis results\n",
    "2. Investigate any significant shift-based variations\n",
    "3. Use distance to nearest blasthole as a confidence metric for grade predictions\n",
    "4. Regular validation of sensor predictions against laboratory assays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}